<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<html>
<head>
    <title>SLfM</title>
    <!-- <meta property="og:image" content=images/teaser.png"/> Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <meta property="og:title" content="Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation" />
    <meta property="og:description" content="Z. Chen, S. Qian, A. Owens." />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- webpage template-->
    <link rel="stylesheet" href="assets/css/style.css">
    <!-- model-viewer css -->
    <link rel="stylesheet" href="assets/css/demo-style.css">

</head>

<body>
    <br>
    <!-- <center> -->
    <center>
        <span style="font-size:38px">Sound Localization from Motion:<br>Jointly Learning Sound Direction and Camera Rotation</span>
    </center>
    <br><br>
    <table align=center width=60%>
        <tr>
            <td align=center width=33%>
                <center>
                    <span style="font-size:20px"><a href="https://ificl.github.io/">Ziyang Chen</a></span>
                </center>
            </td>
            <td align=center width=33%>
                <center>
                    <span style="font-size:20px"><a href="https://jasonqsy.github.io/">Shengyi Qian</a></span>
                </center>
            </td>
            <td align=center width=33%>
                <center>
                    <span style="font-size:20px"><a href="http://andrewowens.com/">Andrew Owens</a></span>
                </center>
            </td>
        </tr>
    </table>
    <br>
    <table align=center width=60%>
        <tr>
            <td align=center width=100%>
                <center>
                    <span style="font-size:20px">University of Michigan</span>
                </center>
            </td>
        </tr>
    </table>
    <br>
    <table align=center width=60%>
        <tr>
            <td align=center width=100%>
                <center>
                    <span style="font-size:20px">ArXiv 2023</span>
                </center>
            </td>
        </tr>
    </table>
    <br>
    <table align=center width=50%>
        <tr>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://arxiv.org/abs/2303.11329">[Paper]</a></span>
                </center>
            </td>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://github.com/IFICL/SLfM">[Github]</a></span>
                </center>
            </td>
            <!-- <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://www.youtube.com/watch?v=vKlN1vxJtLg&ab_channel=ZiyangChen">[Talk]</a></span>
                </center>
            </td>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://www.dropbox.com/s/mn7b884xlmsu213/Stereocrw-public.key?dl=0">[Slides]</a></span>
                </center>
            </td>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="./docs/poster-4119.pdf">[Poster]</a></span>
                </center>
            </td> -->
        </tr>
    </table>

    <br>

    <table align=center width=80%>
        <tr>
            <td width=100%>
                <center>
                    <img src="images/method.png" width="100%"></img><br>
                </center>
            </td>
        </tr>
        <tr>
        <td width=95%>
            <center>
                <span style="font-size:14px"><i> </i> 
            </center>
        </td>
        </tr>
    </table>
    <br>
    <hr>

    <table align=center width=80%>
        <center><h1>Abstract</h1></center>
        <tr>
            <td>
             The images and sounds that we perceive undergo subtle but geometrically consistent changes as we rotate our heads. In this paper, we use these cues to solve a problem we call <b>Sound Localization from Motion</b> (SLfM): jointly estimating camera rotation and localizing sound sources. We learn to solve these tasks solely through self-supervision. A visual model predicts camera rotation from a pair of images, while an audio model predicts the direction of sound sources from binaural sounds. We train these models to generate predictions that agree with one another. At test time, the models can be deployed independently. To obtain a feature representation that is well-suited to solving this challenging problem, we also propose a method for learning an audio-visual representation through cross-view binauralization: estimating binaural sound from one view, given images and sound from another. Our model can successfully estimate accurate rotations on both real and synthetic scenes, and localize sound sources with accuracy competitive with state-of-the-art self-supervised approaches. 
            <br><br>
            </td>
        </tr>
    </table>
    <br><br>
    <hr>


    <!-- <table align=center width=100%>
        <center>
            <h1>Talk</h1>
        </center>
        <div>
            <p align="center">
            <iframe style="width: 70%; height: 50%; min-width:300px" src="https://www.youtube.com/embed/vKlN1vxJtLg" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
        </div>
    </table>
    <br><br>
    <hr> -->

    <table align=center width=80%>
        <center>
            <h1>Qualitative Results</h1>
        </center>
        <tr>
            <td width=100%>
                <center>
                    <h3> Predictions on HM3D-SS </h3>
                </center>
                <p align="center">
                
                    <iframe style="width: 100%; height: 70%; min-width:900px; min-height: 300px;"
                        src="https://www.youtube.com/embed/Wg0OxSX6akk" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                </p>
                
            </td>
        </tr>
        <tr>
            <td width=100%>
            <div>
                <center>
                    <br><br>
                    <h3> Real-world Demo </h3>
                </center>
                <p align="center">
                
                <iframe style="width: 70%; height: 70%; min-width:600px; min-height: 550px;" src="https://www.youtube.com/embed/rMsl3E5zxhU" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </p>
            </div>
            </td>
        </tr>
    </table>
    <br><br>
    <hr>


    <table align=center width=63%>
        <center><h1>Paper and Supplementary Material</h1></center>
        <tr>
            <td><a href="https://arxiv.org/abs/2303.11329"><img class="layered-paper-big" style="height:175px" src="./images/paper.png"/></a></td>
            <td><span style="font-size:14pt">Ziyang Chen, Shengyi Qian, Andrew Owens.<br>
                <b>Sound Localization from Motion:<br>
                    Jointly Learning Sound Direction and Camera Rotation.</b><br>
                ArXiv 2023.
                <br><br>
                (<a href="https://arxiv.org/abs/2303.11329">ArXiv</a>)<br>
            </td>
        </tr>
    </table>
    <br>

    <table align=center width=60%>
        <tr>
            <td><span style="font-size:14pt"><center>
                <a href="./docs/bibtex.txt">[Bibtex]</a>
            </center></td>
        </tr>
    </table>

    <hr>
    <br>

    <table align=center width=70%>
        <tr>
            <td width=80%>
                <left>
                    <center><h1>Acknowledgements</h1></center>
                    This work was funded in part by DARPA Semafor and Sony. The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. The webpage template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">Colorization</a> project. 
                </left>
            </td>
        </tr>
    </table>

<br>
</body>
</html>